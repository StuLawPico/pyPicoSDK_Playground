"""
Log File Analysis Tool

This tool provides functionality to analyze and gain insights from periodic log files
generated by the streaming application. Log files are NumPy arrays (.npy format)
containing downsampled sample values.

Usage:
    python log_analysis_tool.py <log_file.npy> [options]

    python log_analysis_tool.py <log_file.npy> --plot --stats
"""

import argparse
import json
import os
import sys
from datetime import datetime
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np


def load_log_file(file_path: str) -> np.ndarray:
    """
    Load a log file (.npy format) and return the data array.

    Args:
        file_path: Path to the .npy log file

    Returns:
        numpy array containing the logged sample values

    Raises:
        FileNotFoundError: If file doesn't exist
        ValueError: If file cannot be loaded or is invalid
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Log file not found: {file_path}")

    try:
        data = np.load(file_path)

        # Validate data
        if not isinstance(data, np.ndarray):
            raise ValueError(f"File does not contain a numpy array: {file_path}")

        # Flatten if multi-dimensional
        data = data.flatten()

        # Check for empty array
        if len(data) == 0:
            raise ValueError(f"Log file is empty: {file_path}")

        return data

    except Exception as exc:  # noqa: BLE001
        raise ValueError(f"Error loading log file {file_path}: {exc}") from exc


def load_log_metadata(log_file: str) -> dict | None:
    """
    Load sidecar metadata JSON for a given log file, if present.

    The metadata is written by the streaming application as
    `<log_file>_metadata.json` and contains hardware, downsampling,
    and logging settings captured at log creation time.
    """
    base, _ = os.path.splitext(log_file)
    metadata_path = base + "_metadata.json"

    if not os.path.exists(metadata_path):
        return None

    try:
        with open(metadata_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as exc:  # noqa: BLE001
        print(f"Warning: Could not load metadata file {metadata_path}: {exc}")
        return None


def validate_data(data: np.ndarray) -> dict:
    """
    Validate data array and report any issues.

    Args:
        data: numpy array to validate

    Returns:
        dict with validation results and statistics
    """
    results = {
        "valid": True,
        "total_samples": len(data),
        "has_nan": False,
        "has_inf": False,
        "nan_count": 0,
        "inf_count": 0,
        "finite_count": 0,
        "issues": [],
    }

    # Check for NaN values
    nan_mask = np.isnan(data)
    results["has_nan"] = np.any(nan_mask)
    results["nan_count"] = int(np.sum(nan_mask))

    # Check for infinite values
    inf_mask = np.isinf(data)
    results["has_inf"] = np.any(inf_mask)
    results["inf_count"] = int(np.sum(inf_mask))

    # Count finite values
    finite_mask = np.isfinite(data)
    results["finite_count"] = int(np.sum(finite_mask))

    # Report issues
    if results["has_nan"]:
        results["issues"].append(f"Found {results['nan_count']} NaN values")
        results["valid"] = False

    if results["has_inf"]:
        results["issues"].append(f"Found {results['inf_count']} infinite values")
        results["valid"] = False

    if results["finite_count"] == 0:
        results["issues"].append("No finite values found in data")
        results["valid"] = False

    return results


def calculate_statistics(data: np.ndarray) -> dict:
    """
    Calculate core statistics for the log data.

    Args:
        data: numpy array of sample values

    Returns:
        dict with statistical measures
    """
    # Use only finite values for statistics
    finite_data = data[np.isfinite(data)]

    if len(finite_data) == 0:
        return {
            "error": "No finite values available for statistics",
        }

    stats = {
        "count": int(len(finite_data)),
        "mean": float(np.mean(finite_data)),
        "std": float(np.std(finite_data)),
        "variance": float(np.var(finite_data)),
        "min": float(np.min(finite_data)),
        "max": float(np.max(finite_data)),
        "median": float(np.median(finite_data)),
        "range": float(np.max(finite_data) - np.min(finite_data)),
    }

    return stats


def detect_anomalies(
    data: np.ndarray,
    method: str = "iqr",
    threshold: float = 3.0,
) -> dict:
    """
    Detect anomalies in the data using various methods.

    Args:
        data: numpy array of sample values
        method: Method to use ('iqr', 'zscore', 'percentile')
        threshold: Threshold parameter for detection

    Returns:
        dict with anomaly detection results
    """
    finite_data = data[np.isfinite(data)]

    if len(finite_data) == 0:
        return {
            "error": "No finite values available for anomaly detection",
        }

    results: dict = {
        "method": method,
        "threshold": threshold,
        "anomaly_indices": [],
        "anomaly_count": 0,
        "anomaly_percentage": 0.0,
    }

    if method == "iqr":
        # Interquartile Range method
        q25 = np.percentile(finite_data, 25)
        q75 = np.percentile(finite_data, 75)
        iqr = q75 - q25
        lower_bound = q25 - threshold * iqr
        upper_bound = q75 + threshold * iqr

        anomaly_mask = (finite_data < lower_bound) | (finite_data > upper_bound)
        results["lower_bound"] = float(lower_bound)
        results["upper_bound"] = float(upper_bound)

    elif method == "zscore":
        # Z-score method
        mean = np.mean(finite_data)
        std = np.std(finite_data)

        if std > 0:
            z_scores = np.abs((finite_data - mean) / std)
            anomaly_mask = z_scores > threshold
            results["mean"] = float(mean)
            results["std"] = float(std)
        else:
            anomaly_mask = np.zeros_like(finite_data, dtype=bool)

    elif method == "percentile":
        # Percentile-based method
        lower_bound = np.percentile(finite_data, threshold)
        upper_bound = np.percentile(finite_data, 100 - threshold)
        anomaly_mask = (finite_data < lower_bound) | (finite_data > upper_bound)
        results["lower_bound"] = float(lower_bound)
        results["upper_bound"] = float(upper_bound)

    else:
        return {"error": f"Unknown method: {method}"}

    # Get indices of anomalies (in original data array)
    finite_indices = np.where(np.isfinite(data))[0]
    results["anomaly_indices"] = finite_indices[anomaly_mask].tolist()
    results["anomaly_count"] = int(np.sum(anomaly_mask))
    results["anomaly_percentage"] = (
        100.0 * results["anomaly_count"] / len(finite_data)
    )

    return results


def create_plots(
    data: np.ndarray,
    output_dir: str | None = None,
    show_plots: bool = True,
    stats: dict | None = None,
    metadata: dict | None = None,
) -> None:
    """
    Create visualization plots for the log data.

    Args:
        data: numpy array of sample values
        output_dir: Directory to save plots (None = don't save)
        show_plots: Whether to display plots interactively
        stats: Optional statistics dict to display on plot
    """
    # Ensure we use a clean, default Matplotlib style independent of the main app
    plt.rcdefaults()
    plt.style.use("default")

    finite_data = data[np.isfinite(data)]

    # If no statistics dict was provided, compute basic stats here so the
    # figure always has a summary box, even when the tool is called with
    # only --plot (e.g. from the GUI).
    if stats is None:
        computed_stats = calculate_statistics(data)
        if "error" not in computed_stats:
            stats = computed_stats

    if len(finite_data) == 0:
        print("Warning: No finite values to plot")
        return

    # Create figure with subplots (2 rows, 1 column - full horizontal layout)
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    # Explicitly use light background to avoid inheriting dark themes
    fig.patch.set_facecolor("white")
    for ax in axes:
        ax.set_facecolor("white")
    fig.suptitle(
        "Log File Analysis - Visualizations",
        fontsize=14,
        fontweight="bold",
    )

    # 1. Time series plot (index-based) - top plot
    ax1 = axes[0]
    # Plot with explicit markers so individual logged samples are visible
    ax1.plot(
        finite_data,
        marker="x",
        markersize=3,
        linestyle="-",
        linewidth=0.5,
        alpha=0.8,
    )
    ax1.set_xlabel("Sample Index")
    ax1.set_ylabel("Sample Value")
    ax1.set_title("Time Series (Sample Values vs Index)")
    ax1.grid(True, alpha=0.3)

    # 2. Histogram - bottom plot
    ax2 = axes[1]
    ax2.hist(finite_data, bins=50, edgecolor="black", alpha=0.7)
    ax2.set_xlabel("Sample Value")
    ax2.set_ylabel("Frequency")
    ax2.set_title("Distribution Histogram")
    ax2.grid(True, alpha=0.3)

    # Add metadata text box if metadata from log creation is available
    if metadata:
        hw = metadata.get("hardware_settings", {}) or {}
        ds = metadata.get("downsampling_settings", {}) or {}
        log_cfg = metadata.get("logging_settings", {}) or {}
        data_fmt = metadata.get("data_format", {}) or {}

        meta_lines: list[str] = ["Log Metadata:"]
        if "log_start_time" in metadata:
            meta_lines.append(f"Start: {metadata['log_start_time']}")
        if hw:
            channel = hw.get("channel")
            if channel is not None:
                meta_lines.append(f"Channel: {channel}")
            if "adc_sample_rate_hz" in hw:
                meta_lines.append(f"ADC rate: {hw['adc_sample_rate_hz']} Hz")
            if "sample_interval" in hw and "sample_interval_units" in hw:
                meta_lines.append(
                    "Sample interval: "
                    f"{hw['sample_interval']} {hw['sample_interval_units']}"
                )
        if ds:
            ratio = ds.get("ratio")
            mode = ds.get("mode")
            if ratio is not None:
                meta_lines.append(
                    f"Downsampling: {ratio}:1"
                    f"{' ' + str(mode) if mode is not None else ''}"
                )
            if "downsampled_rate_hz" in ds:
                meta_lines.append(f"Downsampled rate: {ds['downsampled_rate_hz']} Hz")
        if log_cfg:
            if "log_rate_seconds" in log_cfg:
                meta_lines.append(f"Log interval: {log_cfg['log_rate_seconds']} s")
            if "sample_selection" in log_cfg:
                meta_lines.append(f"Sample selection: {log_cfg['sample_selection']}")
        if data_fmt:
            units = data_fmt.get("units")
            if units:
                meta_lines.append(f"Units: {units}")

        if len(meta_lines) > 1:
            meta_text = "\n".join(meta_lines)
            # Place metadata box in upper-left to avoid overlapping stats box
            fig.text(
                0.02,
                0.98,
                meta_text,
                verticalalignment="top",
                horizontalalignment="left",
                bbox=dict(boxstyle="round", facecolor="lightgrey", alpha=0.8),
                fontsize=9,
                family="monospace",
            )

    # Add statistics text box on the figure if stats provided
    if stats and "error" not in stats:
        stats_text = (
            "Statistical Summary:\n"
            f"Count: {stats['count']:,}\n"
            f"Mean: {stats['mean']:.6f}\n"
            f"Std Dev: {stats['std']:.6f}\n"
            f"Variance: {stats['variance']:.6f}\n"
            f"Min: {stats['min']:.6f}\n"
            f"Max: {stats['max']:.6f}\n"
            f"Range: {stats['range']:.6f}\n"
            f"Median: {stats['median']:.6f}"
        )

        # Add statistics text box in upper right corner of figure
        fig.text(
            0.98,
            0.98,
            stats_text,
            verticalalignment="top",
            horizontalalignment="right",
            bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.8),
            fontsize=9,
            family="monospace",
        )

    plt.tight_layout()

    # Save plots if requested
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_file = os.path.join(output_dir, f"log_analysis_plots_{timestamp}.png")
        plt.savefig(plot_file, dpi=150, bbox_inches="tight")
        print(f"Plots saved to: {plot_file}")

    # Show plots if requested
    if show_plots:
        plt.show()
    else:
        plt.close()


def generate_report(
    data: np.ndarray,
    file_path: str,
    output_file: str | None = None,
) -> None:
    """
    Generate a comprehensive text report of the analysis.

    Args:
        data: numpy array of sample values
        file_path: Path to the original log file
        output_file: Path to save report (None = print to stdout)
    """
    # Load and validate data
    validation = validate_data(data)
    stats = calculate_statistics(data)
    anomalies = detect_anomalies(data, method="iqr", threshold=3.0)

    # Generate report text
    report_lines: list[str] = []
    report_lines.append("=" * 70)
    report_lines.append("LOG FILE ANALYSIS REPORT")
    report_lines.append("=" * 70)
    report_lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Log File: {file_path}")
    report_lines.append(f"File Size: {os.path.getsize(file_path) / 1024:.2f} KB")
    report_lines.append("")

    # Data validation
    report_lines.append("-" * 70)
    report_lines.append("DATA VALIDATION")
    report_lines.append("-" * 70)
    report_lines.append(f"Total Samples: {validation['total_samples']:,}")
    report_lines.append(f"Finite Values: {validation['finite_count']:,}")
    report_lines.append(f"NaN Values: {validation['nan_count']:,}")
    report_lines.append(f"Infinite Values: {validation['inf_count']:,}")
    if validation["issues"]:
        report_lines.append("Issues Found:")
        for issue in validation["issues"]:
            report_lines.append(f"  - {issue}")
    else:
        report_lines.append("✓ Data validation passed")
    report_lines.append("")

    # Statistics
    if "error" not in stats:
        report_lines.append("-" * 70)
        report_lines.append("STATISTICAL SUMMARY")
        report_lines.append("-" * 70)
        report_lines.append(f"Count: {stats['count']:,}")
        report_lines.append(f"Mean: {stats['mean']:.6f}")
        report_lines.append(f"Std Dev: {stats['std']:.6f}")
        report_lines.append(f"Variance: {stats['variance']:.6f}")
        report_lines.append(f"Min: {stats['min']:.6f}")
        report_lines.append(f"Max: {stats['max']:.6f}")
        report_lines.append(f"Range: {stats['range']:.6f}")
        report_lines.append(f"Median: {stats['median']:.6f}")
        report_lines.append("")

    # Anomaly detection
    if "error" not in anomalies:
        report_lines.append("-" * 70)
        report_lines.append("ANOMALY DETECTION")
        report_lines.append("-" * 70)
        report_lines.append(
            f"Method: {anomalies['method'].upper()} (threshold={anomalies['threshold']})",
        )
        report_lines.append(
            "Anomalies Found: "
            f"{anomalies['anomaly_count']:,}"
            f" ({anomalies['anomaly_percentage']:.2f}%)",
        )
        if "lower_bound" in anomalies:
            report_lines.append(
                "Normal Range: "
                f"[{anomalies['lower_bound']:.6f}, {anomalies['upper_bound']:.6f}]",
            )
        if anomalies["anomaly_count"] > 0 and len(anomalies["anomaly_indices"]) <= 20:
            report_lines.append("Anomaly Indices (first 20):")
            for idx in anomalies["anomaly_indices"][:20]:
                report_lines.append(f"  - Index {idx}: value = {data[idx]:.6f}")
        report_lines.append("")

    report_lines.append("=" * 70)
    report_text = "\n".join(report_lines)

    # Output report
    if output_file:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(report_text)
        print(f"Report saved to: {output_file}")
    else:
        print(report_text)


def main() -> None:
    """Main entry point for the log analysis tool."""
    parser = argparse.ArgumentParser(
        description="Analyze periodic log files from streaming application",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic analysis with plots
  python log_analysis_tool.py data.npy --plot

  # Generate statistics and report
  python log_analysis_tool.py data.npy --stats --export report.txt

  # Full analysis with all features
  python log_analysis_tool.py data.npy --plot --stats --anomalies --export report.txt
        """,
    )

    parser.add_argument("log_file", type=str, help="Path to .npy log file to analyze")
    parser.add_argument(
        "--plot",
        action="store_true",
        help="Generate visualization plots",
    )
    parser.add_argument(
        "--stats",
        action="store_true",
        help="Calculate and display statistics",
    )
    parser.add_argument(
        "--anomalies",
        action="store_true",
        help="Detect anomalies in data",
    )
    parser.add_argument(
        "--export",
        type=str,
        metavar="FILE",
        help="Export report to file",
    )
    parser.add_argument(
        "--no-show",
        action="store_true",
        help="Don't display plots interactively",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        metavar="DIR",
        help="Directory to save plots",
    )

    args = parser.parse_args()

    # Load log file
    try:
        print(f"Loading log file: {args.log_file}")
        data = load_log_file(args.log_file)
        print(f"✓ Loaded {len(data):,} samples")
    except Exception as exc:  # noqa: BLE001
        print(f"Error: {exc}", file=sys.stderr)
        sys.exit(1)

    # Validate data
    validation = validate_data(data)
    if validation["issues"]:
        print("Warning: Data validation issues found:")
        for issue in validation["issues"]:
            print(f"  - {issue}")

    # Generate statistics
    stats: dict | None = None
    if args.stats or args.export:
        stats = calculate_statistics(data)
        if "error" not in stats:
            print("\nStatistical Summary:")
            print(f"  Count: {stats['count']:,}")
            print(f"  Mean: {stats['mean']:.6f}")
            print(f"  Std Dev: {stats['std']:.6f}")
            print(f"  Min: {stats['min']:.6f}")
            print(f"  Max: {stats['max']:.6f}")
            print(f"  Median: {stats['median']:.6f}")
            print(f"  Range: {stats['range']:.6f}")
        else:
            print(f"  {stats['error']}")
            stats = None

    # Detect anomalies
    anomalies: dict | None = None
    if args.anomalies:
        anomalies = detect_anomalies(data, method="iqr", threshold=3.0)
        if "error" not in anomalies:
            print(f"\nAnomaly Detection ({anomalies['method'].upper()}):")
            print(
                "  Anomalies Found: "
                f"{anomalies['anomaly_count']:,}"
                f" ({anomalies['anomaly_percentage']:.2f}%)",
            )
        else:
            print(f"  {anomalies['error']}")
            anomalies = None

    # Create plots
    if args.plot:
        print("\nGenerating plots...")
        metadata = load_log_metadata(args.log_file)
        create_plots(
            data,
            output_dir=args.output_dir,
            show_plots=not args.no_show,
            stats=stats,
            metadata=metadata,
        )

    # Generate report
    if args.export:
        print("\nGenerating report...")
        # Use existing stats/anomalies if present, otherwise recompute
        if stats is None:
            stats = calculate_statistics(data)
        if anomalies is None:
            anomalies = detect_anomalies(data, method="iqr", threshold=3.0)
        generate_report(data, args.log_file, output_file=args.export)

    # If no specific action requested, do basic analysis
    if not (args.plot or args.stats or args.anomalies or args.export):
        print("\nNo specific analysis requested. Showing basic statistics:")
        stats = calculate_statistics(data)
        if "error" not in stats:
            print(f"  Samples: {stats['count']:,}")
            print(f"  Mean: {stats['mean']:.6f}")
            print(f"  Std Dev: {stats['std']:.6f}")
            print(f"  Range: [{stats['min']:.6f}, {stats['max']:.6f}]")
        print("\nUse --help to see available options")


if __name__ == "__main__":
    main()



